
from flask import Flask, request, jsonify
import os
import gc
import time
import torch
import warnings
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorWithPadding
from datasets import load_dataset

app = Flask(__name__)

# Add CORS headers manually
@app.after_request
def after_request(response):
    response.headers.add('Access-Control-Allow-Origin', '*')
    response.headers.add('Access-Control-Allow-Headers', 'Content-Type,Authorization')
    response.headers.add('Access-Control-Allow-Methods', 'GET,PUT,POST,DELETE,OPTIONS')
    return response

# Suppress MPS memory warning
warnings.filterwarnings("ignore", message="'pin_memory' argument is set as true but not supported on")

# Set environment variable for MPS memory 
os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.0"

# Device setup - Force CPU to avoid MPS issues
device = torch.device("cpu")
torch.backends.mps.is_available = lambda: False  # Disable MPS completely

# Global variables for model and dataset
model = None
tokenizer = None
dataset = None
is_fine_tuned = False

def initialize_model():
    """Initialize the model and tokenizer"""
    global model, tokenizer
    print("ðŸ”„ Loading TinyLlama model and tokenizer...")
    model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(model_id)
    model.config.use_cache = False
    model.to(device)
    model.gradient_checkpointing_enable()
    print("âœ… Model and tokenizer loaded successfully!")

def load_medical_dataset():
    """Load and prepare the medical dataset"""
    global dataset
    print("ðŸ“š Loading medical dataset...")
    dataset = load_dataset("lavita/ChatDoctor-HealthCareMagic-100k", split="train").select(range(10))
    print(f"âœ… Dataset loaded with {len(dataset)} samples!")

def preprocess_data(example):
    """Preprocess data for training"""
    max_length = 48
    input_text = example.get('input', '') or ''
    prompt = f"Instruction: {example['instruction']}\n"
    if input_text.strip():
        prompt += f"Input: {input_text}\n"
    prompt += "Answer:"
    answer = example['output']
    full_text = prompt + " " + answer
    tokenized = tokenizer(full_text, truncation=True, padding="max_length", max_length=max_length, return_tensors="pt")
    labels = tokenized["input_ids"].clone()
    prompt_tokenized = tokenizer(prompt, truncation=True, max_length=max_length, return_tensors="pt")
    prompt_len = len(prompt_tokenized["input_ids"][0])
    labels[0, :prompt_len] = -100
    return {
        "input_ids": tokenized["input_ids"].squeeze(),
        "attention_mask": tokenized["attention_mask"].squeeze(),
        "labels": labels.squeeze()
    }

def fine_tune_model():
    """Fine-tune the model on medical data"""
    global model, tokenizer, dataset, is_fine_tuned
    checkpoint_dir = "./tinyllama-chatdoctor-checkpoint"
    if os.path.exists(checkpoint_dir) and os.path.isdir(checkpoint_dir):
        print("ðŸ”„ Loading existing fine-tuned model...")
        try:
            model = AutoModelForCausalLM.from_pretrained(checkpoint_dir)
            model.to(device)
            is_fine_tuned = True
            print("âœ… Fine-tuned model loaded successfully!")
            return
        except Exception as e:
            print(f"âš ï¸ Error loading existing model: {e}")
            print("ðŸ”„ Starting fresh fine-tuning...")
    print("ðŸ”§ Starting fine-tuning process...")
    tokenized_dataset = dataset.map(preprocess_data, remove_columns=dataset.column_names)
    training_args = TrainingArguments(
        output_dir=checkpoint_dir,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=1,
        num_train_epochs=1,
        learning_rate=3e-5,
        warmup_steps=1,
        logging_steps=1,
        save_strategy="epoch",
        report_to="none",
        fp16=False,
        dataloader_drop_last=True,
        remove_unused_columns=False,
        dataloader_num_workers=0,
        save_total_limit=1
    )
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8, return_tensors="pt")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator
    )
    print("ðŸš€ Training started...")
    trainer.train()
    print("ðŸ’¾ Saving fine-tuned model...")
    trainer.save_model()
    gc.collect()
    is_fine_tuned = True
    print("âœ… Fine-tuning completed successfully!")

def initialize_system():
    """Initialize the entire system"""
    print("ðŸš€ Initializing TinyLlama Medical Chatbot...")
    initialize_model()
    load_medical_dataset()
    print("ðŸŽ‰ System initialization complete! Ready to fine-tune on first request.")

# Initialize system on startup (without fine-tuning)
initialize_system()

# Serve index.html from the root directory
@app.route('/')
def serve_ui():
    try:
        with open('index.html', 'r', encoding='utf-8') as file:
            return file.read()
    except FileNotFoundError:
        return "index.html not found. Please make sure the file exists in the root directory.", 404
    except IOError as e:
        return f"Error reading index.html: {str(e)}", 500

# Health check endpoint
@app.route('/health')
def health_check():
    return '', 200  # Returns an empty response with 200 OK status

@app.route('/status', methods=['GET'])
def get_status():
    """Get system status"""
    return jsonify({
        "status": "ready" if is_fine_tuned else "not_ready",
        "message": "Model is fine-tuned and ready for inference" if is_fine_tuned else "Model is not fine-tuned yet",
        "dataset_size": len(dataset) if dataset else 0
    })

@app.route('/infer', methods=['POST'])
def infer():
    """Generate medical advice"""
    global is_fine_tuned
    if not is_fine_tuned:
        fine_tune_model()
    try:
        if not is_fine_tuned:
            return jsonify({"detail": "Model is not fine-tuned yet. Please wait for initialization to complete."}), 503
        data = request.get_json()
        if not data:
            return jsonify({"detail": "No JSON data provided"}), 400
        instruction = data.get('instruction', '').strip()
        input_text = data.get('input_text', '').strip()
        if not instruction:
            return jsonify({"detail": "Instruction field is required"}), 400
        prompt = f"Instruction: {instruction}\n"
        if input_text:
            prompt += f"Input: {input_text}\n"
        prompt += "Answer:"
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=48)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        model.eval()
        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=30,
                do_sample=False,
                temperature=1.0,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        end_time = time.time()
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_answer = response[len(prompt):].strip()
        actual_answer = "No reference answer available in dataset."
        try:
            matching_examples = dataset.filter(
                lambda x: x['instruction'].lower() == instruction.lower() and 
                         x.get('input', '').lower() == input_text.lower()
            )
            if len(matching_examples) > 0:
                actual_answer = matching_examples[0]['output']
        except Exception as e:
            print(f"Error finding reference answer: {e}")
        return jsonify({
            "generated_answer": generated_answer,
            "actual_answer": actual_answer,
            "time_taken": round(end_time - start_time, 2),
            "model_status": "fine_tuned"
        })
    except Exception as e:
        return jsonify({"detail": f"Error during inference: {str(e)}"}), 500

@app.errorhandler(404)
def not_found(error):
    return jsonify({"detail": "Endpoint not found"}), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({"detail": "Internal server error"}), 500
